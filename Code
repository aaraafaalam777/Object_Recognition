import cv2
import os

# Classes
classes = ["mobile", "bottle", "book", "headphone", "mouse"]

# Create folders
for cls in classes:
    os.makedirs(f"dataset/{cls}", exist_ok=True)

cap = cv2.VideoCapture(0)

# ROI coordinates
x, y, w, h = 100, 100, 300, 300

current_class = 0
count = 0
img_size = (128, 128)

print("üì∏ Press 'c' to capture, 'n' for next class, 'q' to quit")

while True:
    ret, frame = cap.read()
    if not ret:
        break

    # Draw ROI
    cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
    cv2.putText(frame, f"Class: {classes[current_class]}  Count: {count}",
                (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
 cv2.imshow("Data Collection", frame)

    key = cv2.waitKey(1) & 0xFF

    if key == ord('c'):  # Capture
        roi = frame[y:y+h, x:x+w]
        roi_resized = cv2.resize(roi, img_size)  # Resize to 128x128
        save_path = f"dataset/{classes[current_class]}/{count}.jpg"
        cv2.imwrite(save_path, roi_resized)
        count += 1
        print(f"‚úÖ Saved {save_path}")

    elif key == ord('n'):  # Next class
        current_class += 1
        count = 0
        if current_class >= len(classes):
            print("üéâ All classes done!")
            break
        print(f"‚û°Ô∏è Moving to class: {classes[current_class]}")

    elif key == ord('q'):  # Quit
        print("üëã Quit by user")
        break

cap.release()
cv2.destroyAllWindows()

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Paths
data_dir = "dataset/"
img_size = (128, 128)
batch_size = 32

# Data augmentation + preprocessing
datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2,   # 80% train, 20% validation
    rotation_range=20,
    zoom_range=0.2,
    horizontal_flip=True
)

train_gen = datagen.flow_from_directory(
    data_dir,
    target_size=img_size,   # ‚úÖ Will just confirm size, no resizing needed
    batch_size=batch_size,
    class_mode='sparse',
    subset='training'
)
val_gen = datagen.flow_from_directory(
    data_dir,
    target_size=img_size,
    batch_size=batch_size,
    class_mode='sparse',
    subset='validation'
)

# CNN Model
model = models.Sequential([
    layers.Conv2D(32, (3,3), activation='relu', input_shape=(128,128,3)),
    layers.MaxPooling2D(2,2),

    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D(2,2),

    layers.Conv2D(128, (3,3), activation='relu'),
    layers.MaxPooling2D(2,2),

    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(5, activation='softmax')   # 5 classes
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
# Training
history = model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=15
)

# Save model
model.save("object_model.h5")

print("‚úÖ Model trained and saved as object_model.h5")
print("Class indices:", train_gen.class_indices)

import cv2
import numpy as np
from tensorflow.keras.models import load_model

# Load trained model
model = load_model("object_model.h5")

# Same order as train_gen.class_indices
labels = ["mobile", "bottle", "book", "headphone", "mouse"]

cap = cv2.VideoCapture(0)
while True:
    ret, frame = cap.read()
    if not ret:
        break

    # Define ROI (Region of Interest)
    x, y, w, h = 100, 100, 300, 300
    roi = frame[y:y+h, x:x+w]

    # Preprocess
    img = cv2.resize(roi, (128,128))
    img = img.astype("float32")/255.0
    img = np.expand_dims(img, axis=0)
 # Prediction
    pred = model.predict(img, verbose=0)
    class_id = np.argmax(pred)

    # Show result
    cv2.putText(frame, labels[class_id], (50,50),
                cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)
    cv2.rectangle(frame, (x,y), (x+w,y+h), (0,255,0), 2)

    cv2.imshow("Object Recognition", frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
